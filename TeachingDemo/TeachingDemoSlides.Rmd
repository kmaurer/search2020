---
title: "K-Nearest Neighbor Predictive Models"
author: "Karsten Maurer"
# institute: "Miami University"
date: "November 11th, 2019"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>

```{r xaringan-themer, include = FALSE}
# devtools::install_github("gadenbuie/xaringanthemer")
library(xaringanthemer)
duo_accent(primary_color = "black",secondary_color = "#cc8a00",header_color = "#cc8a00",title_slide_text_color = "#cc8a00")
```


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(mvtnorm)
library(ggvoronoi)
library(iqbin)
```


### Who am I?

```{r, out.width = "320px", echo=FALSE}
knitr::include_graphics("http://kmaurer.github.io/CV_files/figure-html/unnamed-chunk-1-1.png")
```

#### Karsten Maurer

- Originally from Champlin, MN

- Went to Minnesota, Morris for BA in history and statistics

- Went to Iowa State for PhD in statistics

--

- Currently an Assistant Professor of Statistics at Miami University
  
    + Teaching: Data Visualization, Predictive Modeling, Statistical Programming
    + Research: Statistics Education, Machine Learning, Visualization, Collaborative Consulting



---
### Predictive Modeling Background - Discussion

- What types of predictive models have you encountered in your classes so far?

<br><br><br><br><br><br><br><br><br><br>

--

- What are the two general types of responses that we might try to predict? (Supervised Learning)


---
class: center, middle
## A **K-Nearest Neighbor model** generates predictions based on the responses of the **K most similar** previously observed instances. 

--

... before getting into any notation, let's build the intuition with an exercise.

---

### Predicting Sleep

You have two tasks:

1. Think back to last night. How many hours of sleep did you get? **This is your response value**
<br><br>

2. Ask the 3 people sitting closest to you how much sleep they got. **These are your neighbor's responses**
<br><br><br><br>

--

### Making a prediction about your sleep

Suppose that where you sit in the room (predictor) holds information about how much you sleep (response)
<br><br>

How could we use your neighbor's sleep values to predict for you?

---

### KNN Regression

If $Y_i \in \mathbb{R}$ is your numeric response for instances $i=1,2,...,n$

and $\bf{X_i} \in \mathbb{R}^p$ is your $p$-dimensional numeric predictors
<br><br><br>

--

Suppose we have new instance with predictors $\bf{X_0}$ 
<br><br><br>

--

Define the set of K-Nearest Neighbors based on some distance function $d(.)$ (Typically Euclidean)

$\mathcal{N}_0 = \{i | d({\bf X_i}, {\bf X_0}) \le d({\bf X_{[k]}}, {\bf X_0})\}$

<br><br>


--

Prediction is average of K-neighbors:   $\hat{Y}_0 = \frac{1}{K}\sum_{i \in \mathcal{N}_0} Y_i$

---

### KNN Sleep Regression

$Y$  = sleep hours, $X_1$ =Position Left/Right, $X_2$ =Position Front/Back
<br><br><br>


$\bf{X_0}$ is your seat location
<br><br><br>

$\mathcal{N}_0$  are indices that identify your 3 closest neighbors
<br><br><br>

Our prediction for you is the average sleep from your 3 closest neighbors
<br><br><br>

---

### KNN Algorithm

To predict for any new observation $\bf{X_0}$

1. Compute the distances between $\bf{X_0}$ and all previous observations
2. Sort distances smallest to largest
3. Select K smallest, record which observations these belong to
4. Compute the average response of these observation 

---

```{r KNN_plot_house,echo=F,include=T,eval=T, fig.align='center'}
load("~/GitHub/search2020/TeachingDemo/sold_homes2.Rdata")
head(details_tall)

oxford_real_estate <- details_tall %>% 
  unique() %>%
  spread(key, value) %>%
  right_join(all_sold, by=c(url="url")) %>%
  mutate(lot_units = word(Lot, -1),
         lot_size = ifelse(lot_units=="acres", as.numeric(str_remove(word(Lot,1),","))*43560, as.numeric(str_remove(word(Lot,1),","))),
         sqft = as.numeric(str_remove(word(`Floor size`,1),",")),
         sale_price =  as.numeric(gsub('\\$|,', '',word(`Last sold`,-1))),
         sale_month = word(`Last sold`,1,2)) %>%
  select(street:sale_month, -lot_units)%>%
  na.omit() %>%
  filter(lot_size < 1000000,
         sale_price < 500000)

mytheme <- theme_bw() + 
  theme(panel.grid.minor = element_blank(),
                     panel.border = element_blank())

# plot with X1 breaks
ggplot()+
  geom_point(aes(x=sqft,y=lot_size, color=sale_price),
             data=oxford_real_estate, size=3) +
  scale_color_viridis_c("Price",
                        limits = c(0,500000),
                        breaks = seq(0,500000,by=100000),
                        labels = paste0("$",seq(0,500,by=100),"k"))+
  labs(x="Floor Space (sqft)",
       y="Lot Size (sqft)") +
  mytheme

```


### KNN Algorithm

Start with data with known 

```{r KNN_plot1,echo=F,include=T,eval=T, fig.align='center'}
set.seed(12345)
nsim=45
simbins <- 3
color_steps <- c(-4,-2,0,2,4)
mydata <- data.frame(rmvnorm(nsim, mean=c(0,0), sigma=matrix(c(1,.9,.9,1),byrow=TRUE, nrow=2)))
mydata$Y <- mydata$X1 + mydata$X2 + rnorm(nsim)
point_color <- "black"
line_color <- "black"
mytheme <- theme_bw() + 
  theme(legend.position = "bottom",panel.grid.minor = element_blank(),
                     panel.border = element_blank())

# plot with X1 breaks
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=3) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
    # scale_color_gradient("Y",low="gray95",high="gray15",
    #                     breaks=color_steps, 
    #                     limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

```

---

### KNN Algorithm

1. Compute the distances between $\bf{X_0}$ and all previous observations

```{r IQNN_plot1,echo=F,include=T,eval=T,fig.width=3, fig.height=3.5, out.width='.32\\linewidth'}
set.seed(12345)
nsim=45
simbins <- 3
color_steps <- c(-4,-2,0,2,4)
mydata <- data.frame(rmvnorm(nsim, mean=c(0,0), sigma=matrix(c(1,.9,.9,1),byrow=TRUE, nrow=2)))
mydata$Y <- mydata$X1 + mydata$X2 + rnorm(nsim)
point_color <- "black"
line_color <- "black"
mytheme <- theme_bw() + 
  theme(legend.position = "bottom",panel.grid.minor = element_blank(),
                     panel.border = element_blank())

mybins <- iqbin(data=mydata, bin_cols=c("X1","X2"),nbins=c(simbins, simbins), output="both")
X1_bounds <- unique(c(mybins$bin_def$bin_bounds[,1],mybins$bin_def$bin_bounds[,2]))

# plot with X1 breaks
ggplot()+
  geom_point(aes(x=X1,y=X2, color=Y), data=mydata, size=3) +
  geom_vline(xintercept = X1_bounds, size=.6, color=line_color)+
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
    # scale_color_gradient("Y",low="gray95",high="gray15",
    #                     breaks=color_steps, 
    #                     limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

#iqbin_plot_2d(mybins) rework from here to allow closer match in progression of 3 plots
ggplot()+
  geom_point(aes(x = X1, y = X2,color=Y), data = mydata, size=3)+  
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4), 
            data = data.frame(mybins$bin_def$bin_bounds),
            color =line_color, fill = NA, size=.6) +
  scale_color_gradient2("Y",low="#08519c",mid="gray80",high="#a50f15",
                        midpoint=0,breaks=color_steps,
                        limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
    # scale_color_gradient("Y",low="gray95",high="gray15",
    #                     breaks=color_steps, 
    #                     limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme

# IQNN regression plot
myiqnn <- iqnn(mydata,y="Y", mod_type = "reg", bin_cols=c("X1","X2"),nbins=c(simbins,simbins))
ggplot()+
  geom_rect(aes(xmin = X1, xmax = X2, ymin = X3, ymax = X4, fill=pred), 
            data = data.frame(mybins$bin_def$bin_bounds,pred=myiqnn$bin_stats$pred),
            color =line_color, size=.6) +
  scale_fill_gradient2(expression(hat(Y)),low="#08519c",mid="gray80",high="#a50f15",
                       midpoint=0,breaks=color_steps,
                       limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  # scale_fill_gradient(expression(hat(Y)),low="gray95",high="gray15",
  #                       breaks=color_steps, 
  #                       limits=c(-max(abs(mydata$Y)),max(abs(mydata$Y))))+
  labs(x=expression(X[1]),y=expression(X[2]))+
  mytheme
```


---

### Suggested Supplementary Readings

- James, Witten, Hastie and Tibshirani (2013) Introduction to Statistical Learning. URL <http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf>
- Grolemund and Wickham (2017) R For Data Science. URL <https://r4ds.had.co.nz/>
<br><br>

### Software and Data

- R Core Team, 2019. R: A language and environment for statistical computing. 
- R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
- RStudio Team (2016). RStudio: Integrated Development for R. RStudio,
  Inc., Boston, MA URL http://www.rstudio.com/.
- Wickham, 2019. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.
- Xie (2019). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.22.



---


class: center, middle

# Thanks!

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

